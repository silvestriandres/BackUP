{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')    \n",
    "    \n",
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkContext in module pyspark.context object:\n",
      "\n",
      "class SparkContext(__builtin__.object)\n",
      " |  Main entry point for Spark functionality. A SparkContext represents the\n",
      " |  connection to a Spark cluster, and can be used to create L{RDD} and\n",
      " |  broadcast variables on that cluster.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      " |  \n",
      " |  __exit__(self, type, value, trace)\n",
      " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the context on exit of the with block.\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
      " |      Create a new SparkContext. At least the master and app name should be set,\n",
      " |      either through the named parameters here or through C{conf}.\n",
      " |      \n",
      " |      :param master: Cluster URL to connect to\n",
      " |             (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      " |      :param appName: A name for your job, to display on the cluster web UI.\n",
      " |      :param sparkHome: Location where Spark is installed on cluster nodes.\n",
      " |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n",
      " |             and add to PYTHONPATH.  These can be paths on the local file\n",
      " |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      " |      :param environment: A dictionary of environment variables to set on\n",
      " |             worker nodes.\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. Set 1 to disable batching, 0 to automatically choose\n",
      " |             the batch size based on object sizes, or -1 to use an unlimited\n",
      " |             batch size\n",
      " |      :param serializer: The serializer for RDDs.\n",
      " |      :param conf: A L{SparkConf} object setting Spark properties.\n",
      " |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
      " |             will be instantiated.\n",
      " |      :param jsc: The JavaSparkContext instance (optional).\n",
      " |      :param profiler_cls: A class of custom Profiler used to do profiling\n",
      " |             (default is pyspark.profiler.BasicProfiler).\n",
      " |      \n",
      " |      \n",
      " |      >>> from pyspark.context import SparkContext\n",
      " |      >>> sc = SparkContext('local', 'test')\n",
      " |      \n",
      " |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError:...\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  accumulator(self, value, accum_param=None)\n",
      " |      Create an L{Accumulator} with the given initial value, using a given\n",
      " |      L{AccumulatorParam} helper object to define how to add values of the\n",
      " |      data type if provided. Default AccumulatorParams are used for integers\n",
      " |      and floating-point numbers if you do not provide one. For other types,\n",
      " |      a custom AccumulatorParam can be used.\n",
      " |  \n",
      " |  addFile(self, path, recursive=False)\n",
      " |      Add a file to be downloaded with this Spark job on every node.\n",
      " |      The C{path} passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use\n",
      " |      L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the\n",
      " |      filename to find its download location.\n",
      " |      \n",
      " |      A directory can be given if the recursive option is set to True.\n",
      " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      " |      \n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      >>> path = os.path.join(tempdir, \"test.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"100\")\n",
      " |      >>> sc.addFile(path)\n",
      " |      >>> def func(iterator):\n",
      " |      ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
      " |      ...        fileVal = int(testFile.readline())\n",
      " |      ...        return [x * fileVal for x in iterator]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addPyFile(self, path)\n",
      " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      " |      SparkContext in the future.  The C{path} passed can be either a local\n",
      " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      " |      HTTP, HTTPS or FTP URI.\n",
      " |  \n",
      " |  binaryFiles(self, path, minPartitions=None)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Read a directory of binary files from HDFS, a local file system\n",
      " |      (available on all nodes), or any Hadoop-supported file system URI\n",
      " |      as a byte array. Each file is read as a single record and returned\n",
      " |      in a key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      .. note:: Small files are preferred, large file is also allowable, but\n",
      " |          may cause bad performance.\n",
      " |  \n",
      " |  binaryRecords(self, path, recordLength)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      " |      with the specified numerical format (see ByteBuffer), and the number of\n",
      " |      bytes per record is constant.\n",
      " |      \n",
      " |      :param path: Directory to the input data files\n",
      " |      :param recordLength: The length at which to split the records\n",
      " |  \n",
      " |  broadcast(self, value)\n",
      " |      Broadcast a read-only variable to the cluster, returning a\n",
      " |      L{Broadcast<pyspark.broadcast.Broadcast>}\n",
      " |      object for reading it in distributed functions. The variable will\n",
      " |      be sent to each cluster only once.\n",
      " |  \n",
      " |  cancelAllJobs(self)\n",
      " |      Cancel all jobs that have been scheduled or are running.\n",
      " |  \n",
      " |  cancelJobGroup(self, groupId)\n",
      " |      Cancel active jobs for the specified group. See L{SparkContext.setJobGroup}\n",
      " |      for more information.\n",
      " |  \n",
      " |  dump_profiles(self, path)\n",
      " |      Dump the profile stats into directory `path`\n",
      " |  \n",
      " |  emptyRDD(self)\n",
      " |      Create an RDD that has no partitions or elements.\n",
      " |  \n",
      " |  getConf(self)\n",
      " |  \n",
      " |  getLocalProperty(self, key)\n",
      " |      Get a local property set in this thread, or null if it is missing. See\n",
      " |      L{setLocalProperty}\n",
      " |  \n",
      " |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for sc.sequenceFile.\n",
      " |      \n",
      " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop configuration, passed in as a dict\n",
      " |             (None by default)\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  parallelize(self, c, numSlices=None)\n",
      " |      Distribute a local Python collection to form an RDD. Using xrange\n",
      " |      is recommended if the input represents a range for performance.\n",
      " |      \n",
      " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      " |      [[0], [2], [3], [4], [6]]\n",
      " |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      " |      [[], [0], [], [2], [4]]\n",
      " |  \n",
      " |  pickleFile(self, name, minPartitions=None)\n",
      " |      Load an RDD previously saved using L{RDD.saveAsPickleFile} method.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numSlices=None)\n",
      " |      Create a new RDD of int containing elements from `start` to `end`\n",
      " |      (exclusive), increased by `step` every element. Can be called the same\n",
      " |      way as python's built-in range() function. If called with a single argument,\n",
      " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numSlices: the number of partitions of the new RDD\n",
      " |      :return: An RDD of int\n",
      " |      \n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.range(2, 4).collect()\n",
      " |      [2, 3]\n",
      " |      >>> sc.range(1, 7, 2).collect()\n",
      " |      [1, 3, 5]\n",
      " |  \n",
      " |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n",
      " |      Executes the given partitionFunc on the specified set of partitions,\n",
      " |      returning the result as an array of elements.\n",
      " |      \n",
      " |      If 'partitions' is not specified, this will run over all partitions.\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      " |      [0, 1, 4, 9, 16, 25]\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      " |      [0, 1, 16, 25]\n",
      " |  \n",
      " |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n",
      " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is as follows:\n",
      " |      \n",
      " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      " |             and value Writable classes\n",
      " |          2. Serialization is attempted via Pyrolite pickling\n",
      " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      " |          4. C{PickleSerializer} is used to deserialize pickled objects on the Python side\n",
      " |      \n",
      " |      :param path: path to sequncefile\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      :param keyConverter:\n",
      " |      :param valueConverter:\n",
      " |      :param minSplits: minimum splits in dataset\n",
      " |             (default min(2, sc.defaultParallelism))\n",
      " |      :param batchSize: The number of Python objects represented as a single\n",
      " |             Java object. (default 0, choose batchSize automatically)\n",
      " |  \n",
      " |  setCheckpointDir(self, dirName)\n",
      " |      Set the directory under which RDDs are going to be checkpointed. The\n",
      " |      directory must be a HDFS path if running on a cluster.\n",
      " |  \n",
      " |  setJobDescription(self, value)\n",
      " |      Set a human readable description of the current job.\n",
      " |  \n",
      " |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n",
      " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      " |      different value or cleared.\n",
      " |      \n",
      " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      " |      Application programmers can use this method to group all those jobs together and give a\n",
      " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      " |      \n",
      " |      The application can use L{SparkContext.cancelJobGroup} to cancel all\n",
      " |      running jobs in this group.\n",
      " |      \n",
      " |      >>> import threading\n",
      " |      >>> from time import sleep\n",
      " |      >>> result = \"Not Set\"\n",
      " |      >>> lock = threading.Lock()\n",
      " |      >>> def map_func(x):\n",
      " |      ...     sleep(100)\n",
      " |      ...     raise Exception(\"Task should have been cancelled\")\n",
      " |      >>> def start_job(x):\n",
      " |      ...     global result\n",
      " |      ...     try:\n",
      " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      " |      ...     except Exception as e:\n",
      " |      ...         result = \"Cancelled\"\n",
      " |      ...     lock.release()\n",
      " |      >>> def stop_job():\n",
      " |      ...     sleep(5)\n",
      " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      " |      >>> supress = lock.acquire()\n",
      " |      >>> supress = threading.Thread(target=start_job, args=(10,)).start()\n",
      " |      >>> supress = threading.Thread(target=stop_job).start()\n",
      " |      >>> supress = lock.acquire()\n",
      " |      >>> print(result)\n",
      " |      Cancelled\n",
      " |      \n",
      " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      " |  \n",
      " |  setLocalProperty(self, key, value)\n",
      " |      Set a local property that affects jobs submitted from this thread, such as the\n",
      " |      Spark fair scheduler pool.\n",
      " |  \n",
      " |  setLogLevel(self, logLevel)\n",
      " |      Control our logLevel. This overrides any user-defined log settings.\n",
      " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      " |  \n",
      " |  show_profiles(self)\n",
      " |      Print the profile stats to stdout\n",
      " |  \n",
      " |  sparkUser(self)\n",
      " |      Get SPARK_USER for user who is running SparkContext.\n",
      " |  \n",
      " |  statusTracker(self)\n",
      " |      Return :class:`StatusTracker` object\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Shut down the SparkContext.\n",
      " |  \n",
      " |  textFile(self, name, minPartitions=None, use_unicode=True)\n",
      " |      Read a text file from HDFS, a local file system (available on all\n",
      " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      " |      RDD of Strings.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello world!\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      [u'Hello world!']\n",
      " |  \n",
      " |  union(self, rdds)\n",
      " |      Build the union of a list of RDDs.\n",
      " |      \n",
      " |      This supports unions() of RDDs with different serialized formats,\n",
      " |      although this forces them to be reserialized using the default\n",
      " |      serializer:\n",
      " |      \n",
      " |      >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
      " |      >>> with open(path, \"w\") as testFile:\n",
      " |      ...    _ = testFile.write(\"Hello\")\n",
      " |      >>> textFile = sc.textFile(path)\n",
      " |      >>> textFile.collect()\n",
      " |      [u'Hello']\n",
      " |      >>> parallelized = sc.parallelize([\"World!\"])\n",
      " |      >>> sorted(sc.union([textFile, parallelized]).collect())\n",
      " |      [u'Hello', 'World!']\n",
      " |  \n",
      " |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n",
      " |      Read a directory of text files from HDFS, a local file system\n",
      " |      (available on all nodes), or any  Hadoop-supported file system\n",
      " |      URI. Each file is read as a single record and returned in a\n",
      " |      key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
      " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
      " |      Spark 1.2)\n",
      " |      \n",
      " |      For example, if you have the following files::\n",
      " |      \n",
      " |        hdfs://a-hdfs-path/part-00000\n",
      " |        hdfs://a-hdfs-path/part-00001\n",
      " |        ...\n",
      " |        hdfs://a-hdfs-path/part-nnnnn\n",
      " |      \n",
      " |      Do C{rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")},\n",
      " |      then C{rdd} contains::\n",
      " |      \n",
      " |        (a-hdfs-path/part-00000, its content)\n",
      " |        (a-hdfs-path/part-00001, its content)\n",
      " |        ...\n",
      " |        (a-hdfs-path/part-nnnnn, its content)\n",
      " |      \n",
      " |      .. note:: Small files are preferred, as each file will be loaded\n",
      " |          fully in memory.\n",
      " |      \n",
      " |      >>> dirPath = os.path.join(tempdir, \"files\")\n",
      " |      >>> os.mkdir(dirPath)\n",
      " |      >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
      " |      ...    _ = file1.write(\"1\")\n",
      " |      >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
      " |      ...    _ = file2.write(\"2\")\n",
      " |      >>> textFiles = sc.wholeTextFiles(dirPath)\n",
      " |      >>> sorted(textFiles.collect())\n",
      " |      [(u'.../1.txt', u'1'), (u'.../2.txt', u'2')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(cls, conf=None) from __builtin__.type\n",
      " |      Get or instantiate a SparkContext and register it as a singleton object.\n",
      " |      \n",
      " |      :param conf: SparkConf (optional)\n",
      " |  \n",
      " |  setSystemProperty(cls, key, value) from __builtin__.type\n",
      " |      Set a Java system property, such as spark.executor.memory. This must\n",
      " |      must be invoked before instantiating SparkContext.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  applicationId\n",
      " |      A unique identifier for the Spark application.\n",
      " |      Its format depends on the scheduler implementation.\n",
      " |      \n",
      " |      * in case of local spark app something like 'local-1433865536131'\n",
      " |      * in case of YARN something like 'application_1433865536131_34483'\n",
      " |      \n",
      " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      " |      u'local-...'\n",
      " |  \n",
      " |  defaultMinPartitions\n",
      " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      " |  \n",
      " |  defaultParallelism\n",
      " |      Default level of parallelism to use when not given by user (e.g. for\n",
      " |      reduce tasks)\n",
      " |  \n",
      " |  startTime\n",
      " |      Return the epoch time when the Spark Context was started.\n",
      " |  \n",
      " |  uiWebUrl\n",
      " |      Return the URL of the SparkUI instance started by this SparkContext\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creamos 1000 enteros en una lista\n",
    "integersList = range(1,1001)\n",
    "#integersList\n",
    "len(integersList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Paralelizamos la coleccion utilizando 8 particiones o slices\n",
    "## Esta operacion es una transformacion de datos en un RDD\n",
    "## Dado que Spark usa lazy evaluation, no corren jobs de Spark\n",
    "## hasta el momento\n",
    "integersListRDD = sc.parallelize(integersList, 8)\n",
    "type(integersListRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## podemos ver tambien otra informacion interesante del RDD\n",
    "## el numero de particiones\n",
    "integersListRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(8) ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 []'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## el conjunto de transformaciones que se aplica\n",
    "integersListRDD.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n",
      "\n",
      "class RDD(__builtin__.object)\n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  aggregate(self, zeroValue, seqOp, combOp)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |      \n",
      " |      The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify C{t2}.\n",
      " |      \n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |      \n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |  \n",
      " |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persist this RDD with the default storage level (C{MEMORY_ONLY}).\n",
      " |  \n",
      " |  cartesian(self, other)\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n",
      " |      C{b} is in C{other}.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |  \n",
      " |  checkpoint(self)\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with L{SparkContext.setCheckpointDir()} and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |  \n",
      " |  coalesce(self, numPartitions, shuffle=False)\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |  \n",
      " |  cogroup(self, other, numPartitions=None)\n",
      " |      For each key k in C{self} or C{other}, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in C{self} as\n",
      " |      well as C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Return a list that contains all of the elements in this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |  \n",
      " |  collectAsMap(self)\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting data is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |  \n",
      " |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |      \n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.\n",
      " |      \n",
      " |      Users provide three functions:\n",
      " |      \n",
      " |          - C{createCombiner}, which turns a V into a C (e.g., creates\n",
      " |            a one-element list)\n",
      " |          - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n",
      " |            a list)\n",
      " |          - C{mergeCombiners}, to combine two C's into a single one (e.g., merges\n",
      " |            the lists)\n",
      " |      \n",
      " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
      " |      modify and return their first argument instead of creating a new C.\n",
      " |      \n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |      \n",
      " |      .. note:: V and C can be different -- for example, one might group an RDD of type\n",
      " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> def to_list(a):\n",
      " |      ...     return [a]\n",
      " |      ...\n",
      " |      >>> def append(a, b):\n",
      " |      ...     a.append(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> def extend(a, b):\n",
      " |      ...     a.extend(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
      " |      [('a', [1, 2]), ('b', [1])]\n",
      " |  \n",
      " |  count(self)\n",
      " |      Return the number of elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |  \n",
      " |  countApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |  \n",
      " |  countApproxDistinct(self, relativeSD=0.05)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |      \n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available here\n",
      " |      <http://dx.doi.org/10.1145/2452376.2452456>`_.\n",
      " |      \n",
      " |      :param relativeSD: Relative accuracy. Smaller values create\n",
      " |                         counters that require more space.\n",
      " |                         It must be greater than 0.000017.\n",
      " |      \n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |  \n",
      " |  countByKey(self)\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  countByValue(self)\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |  \n",
      " |  distinct(self, numPartitions=None)\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  filter(self, f)\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Return the first element in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |  \n",
      " |  flatMap(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |  \n",
      " |  flatMapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      >>> x.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |  \n",
      " |  fold(self, zeroValue, op)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      " |      \n",
      " |      The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify C{t2}.\n",
      " |      \n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |  \n",
      " |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |      \n",
      " |      >>> def f(x): print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> def f(iterator):\n",
      " |      ...     for x in iterator:\n",
      " |      ...          print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |  \n",
      " |  fullOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, v) in C{self}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
      " |      (k, (v, None)) if no elements in C{other} have key k.\n",
      " |      \n",
      " |      Similarly, for each element (k, w) in C{other}, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in C{self}, or the pair\n",
      " |      (k, (None, w)) if no elements in C{self} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |  \n",
      " |  getCheckpointFile(self)\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |      \n",
      " |      Not defined if RDD is checkpointed locally.\n",
      " |  \n",
      " |  getNumPartitions(self)\n",
      " |      Returns the number of partitions in RDD\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  getStorageLevel(self)\n",
      " |      Get the RDD's current storage level.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1,2])\n",
      " |      >>> rdd1.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd1.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |  \n",
      " |  glom(self)\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Return an RDD of grouped items.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |  \n",
      " |  groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |      \n",
      " |      .. note:: If you are grouping in order to perform an aggregation (such as a\n",
      " |          sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |          provide much better performance.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |  \n",
      " |  groupWith(self, other, *others)\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |      \n",
      " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |  \n",
      " |  histogram(self, buckets)\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |      \n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) inseration to O(1) per\n",
      " |      element (where n is the number of buckets).\n",
      " |      \n",
      " |      Buckets must be sorted, not contain any duplicates, and have\n",
      " |      at least two elements.\n",
      " |      \n",
      " |      If `buckets` is a number, it will generate buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      " |      will be used.\n",
      " |      \n",
      " |      The return value is a tuple of buckets and histogram.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |  \n",
      " |  id(self)\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |  \n",
      " |  intersection(self, other)\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |      \n",
      " |      .. note:: This method performs a shuffle internally.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  isCheckpointed(self)\n",
      " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      " |  \n",
      " |  isEmpty(self)\n",
      " |      Returns true if and only if the RDD contains no elements at all.\n",
      " |      \n",
      " |      .. note:: an RDD may be empty even when it has at least 1 partition.\n",
      " |      \n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocallyCheckpointed(self)\n",
      " |      Return whether this RDD is marked for local checkpointing.\n",
      " |      \n",
      " |      Exposed for testing.\n",
      " |  \n",
      " |  join(self, other, numPartitions=None)\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      C{self} and C{other}.\n",
      " |      \n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in C{self} and (k, v2) is in C{other}.\n",
      " |      \n",
      " |      Performs a hash join across the cluster.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(x.join(y).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |  \n",
      " |  keyBy(self, f)\n",
      " |      Creates tuples of the elements in this RDD by applying C{f}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> m.collect()\n",
      " |      [1, 3]\n",
      " |  \n",
      " |  leftOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a left outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, v) in C{self}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
      " |      (k, (v, None)) if no elements in C{other} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |  \n",
      " |  localCheckpoint(self)\n",
      " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      " |      \n",
      " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      " |      \n",
      " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      " |      \n",
      " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      " |      with their cached blocks. If you must use both features, you are advised to set\n",
      " |      L{spark.dynamicAllocation.cachedExecutorIdleTimeout} to a high value.\n",
      " |      \n",
      " |      The checkpoint directory set through L{SparkContext.setCheckpointDir()} is not used.\n",
      " |  \n",
      " |  lookup(self, key)\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |      \n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |  \n",
      " |  map(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |  \n",
      " |  mapPartitions(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapPartitionsWithSplit(self, f, preservesPartitioning=False)\n",
      " |      Deprecated: use mapPartitionsWithIndex instead.\n",
      " |      \n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      >>> x.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |  \n",
      " |  max(self, key=None)\n",
      " |      Find the maximum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |  \n",
      " |  mean(self)\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |  \n",
      " |  meanApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  min(self, key=None)\n",
      " |      Find the minimum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |  \n",
      " |  name(self)\n",
      " |      Return the name of this RDD.\n",
      " |  \n",
      " |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash>)\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |      \n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (C{MEMORY_ONLY}).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |  \n",
      " |  pipe(self, command, env=None, checkCode=False)\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |      \n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      [u'1', u'2', u'', u'3']\n",
      " |      \n",
      " |      :param checkCode: whether or not to check the return value of the shell command.\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |      \n",
      " |      :param weights: weights for splits, will be normalized if they don't sum to 1\n",
      " |      :param seed: random seed\n",
      " |      :return: split RDDs in a list\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |  \n",
      " |  reduce(self, f)\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |  \n",
      " |  reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash>)\n",
      " |      Merge the values for each key using an associative and commutative reduce function.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Output will be partitioned with C{numPartitions} partitions, or\n",
      " |      the default parallelism level if C{numPartitions} is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  reduceByKeyLocally(self, func)\n",
      " |      Merge the values for each key using an associative and commutative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  repartition(self, numPartitions)\n",
      " |      Return a new RDD that has exactly numPartitions partitions.\n",
      " |      \n",
      " |      Can increase or decrease the level of parallelism in this RDD.\n",
      " |      Internally, this uses a shuffle to redistribute data.\n",
      " |      If you are decreasing the number of partitions in this RDD, consider\n",
      " |      using `coalesce`, which can avoid performing a shuffle.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |      >>> len(rdd.repartition(2).glom().collect())\n",
      " |      2\n",
      " |      >>> len(rdd.repartition(10).glom().collect())\n",
      " |      10\n",
      " |  \n",
      " |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash>, ascending=True, keyfunc=<function <lambda>>)\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |  \n",
      " |  rightOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, w) in C{other}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in C{self} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |  \n",
      " |  sample(self, withReplacement, fraction, seed=None)\n",
      " |      Return a sampled subset of this RDD.\n",
      " |      \n",
      " |      :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
      " |      :param fraction: expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      :param seed: seed for the random number generator\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |  \n",
      " |  sampleByKey(self, withReplacement, fractions, seed=None)\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |      \n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |  \n",
      " |  sampleStdev(self)\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |  \n",
      " |  sampleVariance(self)\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |  \n",
      " |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
      " |      C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: (None by default)\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
      " |      C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop job configuration, passed in as a dict (None by default)\n",
      " |  \n",
      " |  saveAsPickleFile(self, path, batchSize=10)\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is L{pyspark.serializers.PickleSerializer}, default batch size\n",
      " |      is 10.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
      " |      ['1', '2', 'rdd', 'spark']\n",
      " |  \n",
      " |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |      \n",
      " |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |      \n",
      " |      :param path: path to sequence file\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsTextFile(self, path, compressionCodecClass=None)\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |      \n",
      " |      @param path: path to text file\n",
      " |      @param compressionCodecClass: (None by default) string i.e.\n",
      " |          \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      \n",
      " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |      \n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |      \n",
      " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile2.close()\n",
      " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
      " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |      \n",
      " |      Using compressionCodecClass\n",
      " |      \n",
      " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile3.close()\n",
      " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      >>> b''.join(result).decode('utf-8')\n",
      " |      u'bar\\nfoo\\n'\n",
      " |  \n",
      " |  setName(self, name)\n",
      " |      Assign a name to this RDD.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 2])\n",
      " |      >>> rdd1.setName('RDD1').name()\n",
      " |      u'RDD1'\n",
      " |  \n",
      " |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |  \n",
      " |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |  \n",
      " |  stats(self)\n",
      " |      Return a L{StatCounter} object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |  \n",
      " |  stdev(self)\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |  \n",
      " |  subtract(self, other, numPartitions=None)\n",
      " |      Return each value in C{self} that is not contained in C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtract(y).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  subtractByKey(self, other, numPartitions=None)\n",
      " |      Return each (key, value) pair in C{self} that has no pair with matching\n",
      " |      key in C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtractByKey(y).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  sum(self)\n",
      " |      Add up the elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |  \n",
      " |  sumApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Take the first num elements of the RDD.\n",
      " |      \n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |      \n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |  \n",
      " |  takeOrdered(self, num, key=None)\n",
      " |      Get the N elements from an RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |  \n",
      " |  takeSample(self, withReplacement, num, seed=None)\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |  \n",
      " |  toDF(self, schema=None, sampleRatio=None)\n",
      " |      Converts current :class:`RDD` into a :class:`DataFrame`\n",
      " |      \n",
      " |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n",
      " |      \n",
      " |      :param schema: a :class:`pyspark.sql.types.StructType` or list of names of columns\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :return: a DataFrame\n",
      " |      \n",
      " |      >>> rdd.toDF().collect()\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |  \n",
      " |  toLocalIterator(self)\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  top(self, num, key=None)\n",
      " |      Get the top N elements from an RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: It returns the list sorted in descending order.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |  \n",
      " |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  treeReduce(self, f, depth=2)\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  unpersist(self)\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |  \n",
      " |  values(self)\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> m.collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  variance(self)\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |  \n",
      " |  zip(self, other)\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,5))\n",
      " |      >>> y = sc.parallelize(range(1000, 1005))\n",
      " |      >>> x.zip(y).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |  \n",
      " |  zipWithIndex(self)\n",
      " |      Zips this RDD with its element indices.\n",
      " |      \n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |      \n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |  \n",
      " |  zipWithUniqueId(self)\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |      \n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      L{zipWithIndex}\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  context\n",
      " |      The L{SparkContext} that this RDD was created on.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## para ver mas metodos disponibles del RDD\n",
    "help(integersListRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1609', u'', u'THE SONNETS', u'', u'by William Shakespeare']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creamos el RDD a partir de un archivo de texto\n",
    "shakespeareRDD = sc.textFile('shakespeare.txt',8)\n",
    "## aplicamos una accion para tomar los 15 items del RDD\n",
    "#type(shakespeareRDD)\n",
    "shakespeareRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integersListRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def por_dos(x): return x*2\n",
    "\n",
    "## aplicamos una transformacion map para restar a todos 1\n",
    "## aplicamos la accion take para mostrar 10 resultados\n",
    "## ver generacion de tuplas y uso de funciones de python en pyspark.\n",
    "integersListRDD.map(por_dos).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 0), (4, 2, 1), (9, 3, 2), (16, 4, 3), (25, 5, 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## aplicamos una funcin lambda, y generamos tuplas de a 3 elementos \n",
    "## por cada uno existente previamente.\n",
    "integersListRDD.map(lambda x: (x**2, x, x-1)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 2, 0),\n",
       " (2, 1, 3, 1),\n",
       " (3, 1, 4, 2),\n",
       " (4, 1, 5, 3),\n",
       " (5, 1, 6, 4),\n",
       " (6, 1, 7, 5),\n",
       " (7, 1, 8, 6),\n",
       " (8, 1, 9, 7),\n",
       " (9, 1, 10, 8),\n",
       " (10, 1, 11, 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generamos una tupla que tenga el valor original, el anterior y siguiente\n",
    "## aplicamos una transformacion map para restar a todos 1\n",
    "## aplicamos la accion take para mostrar 10 resultados\n",
    "\n",
    "integersListRDD.map(lambda a: (a, 1, 1+a, a-1)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "## aplicamos una transformacion map para restar a todos 1\n",
    "## aplicamos la accion take para mostrar 10 resultados\n",
    "\n",
    "subRDD = integersListRDD.map(lambda a: a-1)\n",
    "print(subRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## por ejemplo para los RDD con los que estuvimos trabajando\n",
    "\n",
    "integersListRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "124614\n"
     ]
    }
   ],
   "source": [
    "# por ejemplo para los RDD con los que estuvimos trabajando\n",
    "\n",
    "print(integersListRDD.count())\n",
    "print(subRDD.count())\n",
    "print(shakespeareRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "[2]\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "## obtener los numeros pares\n",
    "print(integersListRDD.filter(lambda a: a % 2 == 0).take(10))\n",
    "print(integersListRDD.filter(lambda a: a == 2).take(10))\n",
    "print(integersListRDD.filter(lambda a: a % 2 == 0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1609',\n",
       " u'THE SONNETS',\n",
       " u'by William Shakespeare',\n",
       " u'                     1',\n",
       " u'  From fairest creatures we desire increase,',\n",
       " u\"  That thereby beauty's rose might never die,\",\n",
       " u'  But as the riper should by time decease,',\n",
       " u'  His tender heir might bear his memory:',\n",
       " u'  But thou contracted to thine own bright eyes,',\n",
       " u\"  Feed'st thy light's flame with self-substantial fuel,\",\n",
       " u'  Making a famine where abundance lies,',\n",
       " u'  Thy self thy foe, to thy sweet self too cruel:',\n",
       " u\"  Thou that art now the world's fresh ornament,\",\n",
       " u'  And only herald to the gaudy spring,',\n",
       " u'  Within thine own bud buriest thy content,']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminar lineas vacias de the complete works of shakespeare\n",
    "#shakespeareRDD.take(15)\n",
    "shakespeareRDD.filter(lambda a: a != \"\").take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114976\n",
      "124614\n"
     ]
    }
   ],
   "source": [
    "print(shakespeareRDD.filter(lambda a: a != \"\").count())\n",
    "print(shakespeareRDD.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500500\n",
      "500500\n"
     ]
    }
   ],
   "source": [
    "## la suma es asociativa y conmutativa\n",
    "print(integersListRDD.reduce(lambda a, b: a + b))\n",
    "print(integersListRDD.repartition(8).reduce(lambda a, b: a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477738\n",
      "388388\n"
     ]
    }
   ],
   "source": [
    "## la resta no no asociativa y conmutativa\n",
    "print(integersListRDD.reduce(lambda a, b: a - b))\n",
    "print(integersListRDD.repartition(8).reduce(lambda a, b: a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n"
     ]
    }
   ],
   "source": [
    "## first() devuelve el primer elemento del RDD\n",
    "print(shakespeareRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n"
     ]
    }
   ],
   "source": [
    "## first() devuelve el primer elemento del RDD\n",
    "print(shakespeareRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1000, 999, 998]\n"
     ]
    }
   ],
   "source": [
    "## traemos los 3 elementos mas pequeos\n",
    "print(integersListRDD.takeOrdered(3))\n",
    "## traemos los 3 elementos mas grandes\n",
    "print(integersListRDD.top(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 999, 998]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## se puede utilizar una funcion para fijar el orden en takeOrdered\n",
    "## por ejemplo para revertirlo\n",
    "integersListRDD.takeOrdered(3, lambda a: -a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [1, 3], [1, 4]]\n",
      "[1, 2, 1, 3, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "## Similar a map, pero permite que cada item de entrada se mapee a cero o mas elementos de salida.\n",
    "simpleRDD = sc.parallelize([2, 3, 4])\n",
    "print(simpleRDD.map(lambda x: [1, x]).collect())\n",
    "print(simpleRDD.flatMap(lambda x: [1, x]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "listadelistas = [[1,2],[2,3],[5]]\n",
    "rdd = sc.parallelize(listadelistas)\n",
    "print(rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'fawn', 11), (u'voluble', 2), (u'annoy!', 2), (u'mustachio', 1), (u'four', 114), (u'reproach-', 1), (u'drollery.', 1), (u'conjuring', 1), (u'slew.', 1), (u'increase', 14), (u'Drabbing.', 1), (u'Sergeant.', 1), (u'fig.', 1), (u'out-night', 1), (u'sinking', 2), (u'goats.', 2), (u'self-comparisons,', 1), (u'impertinent', 1), (u'spider.', 1), (u\"suck'd.\", 1), (u'Sennet.', 7), (u'advice;', 2), (u'Lest', 65), (u'Brakenbury,', 3), (u'wilfull', 1), (u'accommodo.', 1), (u'ignomy.', 1), (u'solid', 4), (u\"hack'd\", 7), (u'spoken-', 1), (u'fire!', 5), (u'undertaker.', 1), (u'Provost?', 4), (u'tires', 6), (u\"Broach'd\", 1), (u'rusts', 1), (u'understood:', 1), (u'admire', 4), (u'nature,', 61), (u'Jarteer', 1), (u\"'You\", 5), (u'additions;', 1), (u'sights!', 1), (u'conceive?', 2), (u'cull', 5), (u'up.', 76), (u'Whenever', 1), (u'chimneys', 2), (u'chine', 1), (u'fret,', 1), (u'hers', 13), (u'disseat', 1), (u'quart', 3), (u'Ceres,', 2), (u'finger.', 6), (u\"'veal'\", 1), (u'NOT', 220), (u'bring:', 1), (u'Argo,', 1), (u\"mirth,'\", 1), (u'him-no', 1), (u\"Endur'd\", 2), (u'brought', 178), (u\"cross'd;\", 2), (u'oft;', 2), (u'common.', 6), (u'snip', 2), (u'browse', 1), (u'Ancient,', 2), (u\"hatch'd\", 6), (u'moth', 1), (u'peize', 1), (u'strike', 90), (u'uncertainty,', 1), (u'discontenting', 1), (u'higher,', 3), (u'Charles,', 15), (u'praiseworthy.', 1), (u'successful', 4), (u'gallows;', 2), (u'Veronesa.', 1), (u\"hurlyburly's\", 1), (u'pregnantly', 1), (u'91', 2), (u'brows-', 1), (u'desk,', 1), (u'pursue', 20), (u'Guildenstern?', 1), (u'misfortunes', 2), (u'nourisher', 1), (u'breese', 1), (u'forever?', 1), (u'forefinger,', 1), (u'Just;', 1), (u'Long-lane', 1), (u\"by'r\", 8), (u'well-armed', 1), (u'motion', 47), (u'instructed,', 1), (u'keep,', 8)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "#pairRDD = sc.parallelize([('a', 1), ('a', 2), ('b', 1)])\n",
    "\n",
    "# usamos mapValues para mejorar el formato de impresion\n",
    "#print(pairRDD.reduceByKey(lambda a,b: a+b).collect())\n",
    "\n",
    "# Diferentes formas de sumar por clave\n",
    "# problemas de notacion python3\n",
    "# print(pairRDD.groupByKey().map(lambda x: (x[1],sum(x[2]))).collect())\n",
    "\n",
    "# Using mapValues, which is recommended when they key doesn't change\n",
    "#print(pairRDD.groupByKey().mapValues(lambda x: sum(x)).collect())\n",
    "\n",
    "# reduceByKey is more efficient / scalable\n",
    "#print(pairRDD.reduceByKey(add).collect())\n",
    "\n",
    "print(shakespeareRDD.flatMap(lambda a: a.split()).map(lambda a: (a, 1)).reduceByKey(lambda a,b: a+b).take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.ibiblio.org/gutenberg/etext06\n"
     ]
    }
   ],
   "source": [
    "# generando tuplas para obtener la linea \n",
    "# de maxima longitud de todo el texto\n",
    "\n",
    "result = shakespeareRDD.flatMap(lambda a: a.split()).reduce(lambda a,b: a if len(a) > len(b) else b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================================================================\n",
    "# A PARTIR DE AQU SE EJECUTAN LAS INSTRUCCIONES DEL SEGUNDO EJEMPLO.\n",
    "# ========================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeareRDD = sc.textFile('shakespeare.txt',8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The Project Gutenberg EBook of The Complete Works of William Shakespeare, by',\n",
       " u'William Shakespeare',\n",
       " u'',\n",
       " u'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " u'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " u're-use it under the terms of the Project Gutenberg License included',\n",
       " u'with this eBook or online at www.gutenberg.org',\n",
       " u'',\n",
       " u'** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **',\n",
       " u'**     Please follow the copyright guidelines in this file.     **',\n",
       " u'',\n",
       " u'Title: The Complete Works of William Shakespeare',\n",
       " u'',\n",
       " u'Author: William Shakespeare',\n",
       " u'',\n",
       " u'Posting Date: September 1, 2011 [EBook #100]',\n",
       " u'Release Date: January, 1994',\n",
       " u'',\n",
       " u'Language: English',\n",
       " u'']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeareRDD.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The',\n",
       " u'Project',\n",
       " u'Gutenberg',\n",
       " u'EBook',\n",
       " u'of',\n",
       " u'The',\n",
       " u'Complete',\n",
       " u'Works',\n",
       " u'of',\n",
       " u'William']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD = shakespeareRDD.flatMap(lambda line: line.split())\n",
    "wordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'The',\n",
       "  u'Project',\n",
       "  u'Gutenberg',\n",
       "  u'EBook',\n",
       "  u'of',\n",
       "  u'The',\n",
       "  u'Complete',\n",
       "  u'Works',\n",
       "  u'of',\n",
       "  u'William',\n",
       "  u'Shakespeare,',\n",
       "  u'by'],\n",
       " [u'William', u'Shakespeare'],\n",
       " [],\n",
       " [u'This',\n",
       "  u'eBook',\n",
       "  u'is',\n",
       "  u'for',\n",
       "  u'the',\n",
       "  u'use',\n",
       "  u'of',\n",
       "  u'anyone',\n",
       "  u'anywhere',\n",
       "  u'at',\n",
       "  u'no',\n",
       "  u'cost',\n",
       "  u'and',\n",
       "  u'with'],\n",
       " [u'almost',\n",
       "  u'no',\n",
       "  u'restrictions',\n",
       "  u'whatsoever.',\n",
       "  u'You',\n",
       "  u'may',\n",
       "  u'copy',\n",
       "  u'it,',\n",
       "  u'give',\n",
       "  u'it',\n",
       "  u'away',\n",
       "  u'or'],\n",
       " [u're-use',\n",
       "  u'it',\n",
       "  u'under',\n",
       "  u'the',\n",
       "  u'terms',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'Project',\n",
       "  u'Gutenberg',\n",
       "  u'License',\n",
       "  u'included'],\n",
       " [u'with', u'this', u'eBook', u'or', u'online', u'at', u'www.gutenberg.org'],\n",
       " [],\n",
       " [u'**',\n",
       "  u'This',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'COPYRIGHTED',\n",
       "  u'Project',\n",
       "  u'Gutenberg',\n",
       "  u'eBook,',\n",
       "  u'Details',\n",
       "  u'Below',\n",
       "  u'**'],\n",
       " [u'**',\n",
       "  u'Please',\n",
       "  u'follow',\n",
       "  u'the',\n",
       "  u'copyright',\n",
       "  u'guidelines',\n",
       "  u'in',\n",
       "  u'this',\n",
       "  u'file.',\n",
       "  u'**']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD = shakespeareRDD.map(lambda line: line.split())\n",
    "wordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The',\n",
       " u'Project',\n",
       " u'Gutenberg',\n",
       " u'EBook',\n",
       " u'of',\n",
       " u'The',\n",
       " u'Complete',\n",
       " u'Works',\n",
       " u'of',\n",
       " u'William']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRDD = shakespeareRDD.flatMap(lambda line: line.split())\n",
    "wordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', 1),\n",
       " (u'Project', 1),\n",
       " (u'Gutenberg', 1),\n",
       " (u'EBook', 1),\n",
       " (u'of', 1),\n",
       " (u'The', 1),\n",
       " (u'Complete', 1),\n",
       " (u'Works', 1),\n",
       " (u'of', 1),\n",
       " (u'William', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsCountRDD = wordsRDD.map(lambda word: (word,1))\n",
    "wordsCountRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 23407),\n",
       " (u'I', 19540),\n",
       " (u'and', 18358),\n",
       " (u'to', 15682),\n",
       " (u'of', 15649),\n",
       " (u'a', 12586),\n",
       " (u'my', 10825),\n",
       " (u'in', 9633),\n",
       " (u'you', 9129),\n",
       " (u'is', 7874)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsCountRDD.reduceByKey(lambda a,b: a+b).sortBy(ascending=False,keyfunc=lambda x:x[1]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 23407),\n",
       " (u'I', 19540),\n",
       " (u'and', 18358),\n",
       " (u'to', 15682),\n",
       " (u'of', 15649),\n",
       " (u'a', 12586),\n",
       " (u'my', 10825),\n",
       " (u'in', 9633),\n",
       " (u'you', 9129),\n",
       " (u'is', 7874)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsCountRDD.reduceByKey(lambda a,b: a+b).sortBy(ascending=False,keyfunc=lambda x:x[1]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams(t):\n",
    "    t=t.lower()\n",
    "    return [t[i:i+3] for i in range(0, len(t) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hol', 'ola', 'la ', 'a d', ' da', 'dat', 'ato', 'tos']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams(\"hola datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "anotherShakespeareRDD = sc.textFile('shakespeare.txt',8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramsRDD = anotherShakespeareRDD.flatMap(trigrams).filter(lambda a : a != '   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the',\n",
       " u'he ',\n",
       " u'e p',\n",
       " u' pr',\n",
       " u'pro',\n",
       " u'roj',\n",
       " u'oje',\n",
       " u'jec',\n",
       " u'ect',\n",
       " u'ct ']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigramsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'osy', 15), (u'aln', 4), (u'? t', 262), (u'f; ', 116), (u' 54', 1)]\n"
     ]
    }
   ],
   "source": [
    "trigramsCount = trigramsRDD.map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y)\n",
    "print(trigramsCount.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u' th', 83584), (u'the', 52064), (u'he ', 35146), (u'and', 32713), (u' an', 32680), (u'nd ', 31193), (u' to', 23631), (u'is ', 23165), (u' yo', 22910), (u'you', 22280), (u' he', 20994), (u' of', 19854), (u'to ', 19842), (u' no', 19327), (u' i ', 19146), (u'her', 18983), (u'hat', 18804), (u'll ', 18616), (u'at ', 18108), (u' wi', 17954)]\n"
     ]
    }
   ],
   "source": [
    "trigramsCountSorted = trigramsCount.sortBy(ascending=False,keyfunc=lambda x:x[1])\n",
    "print(trigramsCountSorted.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u' th', 83584),\n",
       " (u'the', 52064),\n",
       " (u'he ', 35146),\n",
       " (u'and', 32713),\n",
       " (u' an', 32680),\n",
       " (u'nd ', 31193),\n",
       " (u' to', 23631),\n",
       " (u'is ', 23165),\n",
       " (u' yo', 22910),\n",
       " (u'you', 22280)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigramsCountSorted.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835997\n",
      "4835997\n"
     ]
    }
   ],
   "source": [
    "totalFrec2 = trigramsCountSorted.map(lambda x: x[1]).reduce(lambda x,y: x+y)\n",
    "totalFrec1 = trigramsRDD.count()\n",
    "print(totalFrec2)\n",
    "print(totalFrec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert totalFrec2 == totalFrec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835997"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalFrec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835997"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalFrec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835997\n"
     ]
    }
   ],
   "source": [
    "print(totalFrec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835997\n"
     ]
    }
   ],
   "source": [
    "print(trigramsRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculando la probabilidad de cada trigrama\n",
    "## print(trigramsCountSorted.take(5))\n",
    "trigramsProb = trigramsCountSorted.map(lambda x: (x[0],round(float(x[1])/totalFrec1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u' th', 0.017),\n",
       " (u'the', 0.011),\n",
       " (u'he ', 0.007),\n",
       " (u'and', 0.007),\n",
       " (u' an', 0.007),\n",
       " (u'nd ', 0.006),\n",
       " (u' to', 0.005),\n",
       " (u'is ', 0.005),\n",
       " (u' yo', 0.005),\n",
       " (u'you', 0.005)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigramsProb.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================\n",
    "# Joins en Apache Spark\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_data = [\n",
    "    (1,'People A'),\n",
    "    (2,'People B'),\n",
    "    (3,'People C'),\n",
    "    (4,'People D'),\n",
    "    (5,'People E')\n",
    "]\n",
    "\n",
    "a = sc.parallelize(people_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'People A'),\n",
       " (2, 'People B'),\n",
       " (3, 'People C'),\n",
       " (4, 'People D'),\n",
       " (5, 'People E')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_data = [(1, 'Subject 1'),\n",
    "               (2, 'Subject 2'),\n",
    "               (200, 'Subject 1500'),\n",
    "               (2, 'Subject 2 Repetido')]\n",
    "\n",
    "b = sc.parallelize(subject_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Subject 1'),\n",
       " (2, 'Subject 2'),\n",
       " (200, 'Subject 1500'),\n",
       " (2, 'Subject 2 Repetido')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('People A', 'Subject 1')),\n",
       " (2, ('People B', 'Subject 2')),\n",
       " (2, ('People B', 'Subject 2 Repetido'))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Inner Join (Join)\n",
    "## Cuando se llama para sets de datos del tipo (K,V) y (K,W) devuelve un set de datos del tipo (K, (V,W)) \n",
    "## con todos los pares de elementos para cada key. (especificamente los que hay en comun \n",
    "## por esa clave en ambos sets de datos)\n",
    "\n",
    "a.join(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(200, (None, 'Subject 1500')),\n",
       " (1, ('People A', 'Subject 1')),\n",
       " (2, ('People B', 'Subject 2')),\n",
       " (2, ('People B', 'Subject 2 Repetido'))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Right Outer Join\n",
    "## Cuando se llama para sets de datos del tipo (K,V) y (K,W) devuelve un set de datos del tipo (K, (V,W)) \n",
    "## asegurandonos que todos los datos del set de datos derecho estaran en el resultado del join.\n",
    "\n",
    "a.rightOuterJoin(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('People A', 'Subject 1')),\n",
       " (2, ('People B', 'Subject 2')),\n",
       " (2, ('People B', 'Subject 2 Repetido')),\n",
       " (3, ('People C', None)),\n",
       " (4, ('People D', None)),\n",
       " (5, ('People E', None))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Left Outer Join\n",
    "## Cuando se llama para sets de datos del tipo (K,V) y (K,W) devuelve un set de datos del tipo (K, (V,W)) \n",
    "## asegurandonos que todos los datos del set de datos izquierdo estaran en el resultado del join.\n",
    "\n",
    "a.leftOuterJoin(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(200, (None, 'Subject 1500')),\n",
       " (1, ('People A', 'Subject 1')),\n",
       " (2, ('People B', 'Subject 2')),\n",
       " (2, ('People B', 'Subject 2 Repetido')),\n",
       " (3, ('People C', None)),\n",
       " (4, ('People D', None)),\n",
       " (5, ('People E', None))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Outer/Full Join\n",
    "## Cuando se llama para sets de datos del tipo (K,V) y (K,W) devuelve un set de datos del tipo (K, (V,W)) \n",
    "## asegurandonos que todos los datos de ambos set de datos estaran aunque no haya match de keys.\n",
    "\n",
    "a.fullOuterJoin(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
